# 中级机器学习小组第八期课程大纲

本仓库包含了课程大纲，包括课程安排、教材和参考资料等。

## 教材

- [The Elements of Statistical Learning](https://hastie.su.domains/Papers/ESLII.pdf)
- [An Introduction to Statistical Learning](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)

## 参考资料

- Hastie的主页：[https://hastie.su.domains/index.html](https://hastie.su.domains/index.html)
- Friedman的主页：[https://jerryfriedman.su.domains](https://jerryfriedman.su.domains/)
- ISL的Python代码：[https://github.com/JWarmenhoven/ISLR-python](https://github.com/JWarmenhoven/ISLR-python)

## 推荐书籍

- Computer-Age Statistical Inference
- Pattern Recognition and Machine Learning (圣经)
- Probabilistic Machine Learning: [https://probml.github.io/pml-book/book1.html](https://probml.github.io/pml-book/book1.html)
- PML: Advanced Topics: [https://probml.github.io/pml-book/book2.html](https://probml.github.io/pml-book/book2.html)

## 公开课（带视频）

1. Stanford ISL (Hastie)：[https://www.edx.org/course/statistical-learning](https://www.edx.org/course/statistical-learning)
2. NTU Machine Learning Foundation and Technics (林轩田)：[https://www.coursera.org/instructor/htlin](https://www.coursera.org/instructor/htlin)
3. Bilibili：机器学习白板推导：[https://www.bilibili.com/video/BV1aE411o7qd](https://www.bilibili.com/video/BV1aE411o7qd)


## Schedule 

Week 1(10.29): Orientation 

### Basic Topics 

Week 2(11.5): Introduction1-Linear Algebra (Xiaonan Peng)

Linear transformation, matrix multiplication, four subspaces, determinant, eigenvalue problem, spectral theorem, SVD, multi-noraml distribution.

Week 3(11.12): Introduction2-Optimization (Xiaonan Peng)

Matrix calculus: vector and matrix derivatives. Convex optimization: convex set and function, first order condition theorem, Lagrange multiplier, KKT condition.

Week 4(11.19): Lecture 1: Linear Regression (Xiaonan Peng)

Bia-variance trade-off. Simple linear regression: LSE, MLE, Bayesian estimator, R^2 and r^2. Multiple linear regression: Closed-form solution: LSE, geometric interptation and MLE, Gauss-Markov thm.
Abnormal diagnostics: non-linearity, correlation of errorsm, outlier, high leverge points, collinearity.

ISL: 3.1-3.5; ESL: ch2, 3.1-3.2

Week 5(11.26): Lecture 2: Linear Classification (Zhongcan Wang)

ISL: 4.1-4.5; ESL: 4.1-4.2, 4.3.1-4.3.2, 4.4 

Week 6(12.3): Lecture 1(continued): Regularization (Xiaonan Peng) 

Ridge and Lasso.

ISL: 6.1-6.2; ESL: 3.3-3.4 

Week 7(12.10): Tutorial 1(Xiaonan Peng) 

Week 8(12.17): Tutorial 2(Zhongcan Wang) 

Week 9 & 10: Break 

Week 11(1.7): Lecture 3: Resampling, model selection: CV and Bootstrap (Hao Ying) 

ISL: 5.1-5.2; ESL: 7.1-7.12, 8.1-8.4 

Week 12(1.14): Lecture 4: Additive Model and Kernel Smoothing Method (Bowei Fan) .14

ISL: 7.1-7.7; ESL: 5.1-5.6, 6.1-6.9 

Week 13: Break

Week 14(1.28): Interview question of regression(Xiaonan Peng)

Week 15(2.4): Tutorial 3(Hao Ying) 

Week 16(2.18): Tutorial 4(Bowei Fan)

Week 17(2.25): Lecture 5: Tree and Random Forest (Zhuangzhuang Han) 

ISL: 8.1, 8.2.1-8.2.2; ESL: 9.1-9.7, 15.1-15.4  

Week 18(3.4): Lecture 6: Adaboost (Zhe Yang) 

ISL: 8.2.3-8.2.5; ESL:10.1-10.14, 16.1-16.3 

Week 19(3.11): Break

Week 20(3.18): Tutorial 5 (Zhuangzhuang Han & Hao Ying) 

Week 21(3.25): Lecture 7: Deep Learning (Jade Tan) 

Week 22(4.1):  Lecture 6(Continued): Gradient Boost (Yang Zhe)

Week 23(4.8): Break

Week 24(4.15): Lecture 8: SVM (Xiaonan Peng) 

Week 25(4.22): Lectuer 7(Continued): AlexNet (Jade Tan) 
